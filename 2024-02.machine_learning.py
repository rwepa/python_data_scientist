"""
file     : 2024-02.machine_learning.py
title    : Day2 Python æ©Ÿå™¨å­¸ç¿’
author   : Ming-Chang Lee
date     : 2024.09.26
email    : alan9956@gmail.com
YouTube  : https://www.youtube.com/@alan9956
RWEPA    : http://rwepa.blogspot.tw/
GitHub   : https://github.com/rwepa
"""

##############################
# 5å¤©ç¨‹å¼èªè¨€æ‡‰ç”¨ä¸»é¡Œ
##############################
# Day1 Python èªè¨€ç°¡ä»‹
# Day2 Python æ©Ÿå™¨å­¸ç¿’ ğŸŒ¸
# Day3 è³‡æ–™æ¢å‹˜æŠ€è¡“ä»‹ç´¹
# Day4 Pythonç¨‹å¼æ‡‰ç”¨å¯¦ä¾‹
# Day5 å¤§æ•¸æ“šç¨‹å¼èªè¨€è©•é‡æ¸¬é©—èˆ‡è§£æ

##############################
# Day2 Python æ©Ÿå™¨å­¸ç¿’
##############################

# å¤§ç¶±
# 1.CRISP-DM è·¨ç”¢æ¥­è³‡æ–™æ¢å‹˜æ¨™æº–ä½œæ¥­æµç¨‹
# 2.æ©Ÿå™¨å­¸ç¿’ç°¡ä»‹
# 3.éç›£ç£å¼å­¸ç¿’èˆ‡ç›£ç£å¼å­¸ç¿’
# 4.ç›£ç£å¼å­¸ç¿’çš„è©•ä¼°
# 5.scikit-learnd æ¨¡çµ„ç°¡ä»‹

##############################
# 1.CRISP-DM è·¨ç”¢æ¥­è³‡æ–™æ¢å‹˜æ¨™æº–ä½œæ¥­æµç¨‹
##############################

# https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining

# è·¨ç”¢æ¥­è³‡æ–™æ¢å‹˜æ¨™æº–ä½œæ¥­æµç¨‹ (CRoss Industry Standard Process for Data Mining)
# CRISP-DMæ˜¯æ–¼1990å¹´èµ·ï¼Œç”±SPSSä»¥åŠNCRå…©å¤§å» å•†åœ¨åˆä½œæˆ´å§†å…‹èŠæ–¯å‹’-è³“å£«(Daimler Benz)çš„
# è³‡æ–™å€‰å„²ä»¥åŠè³‡æ–™æ¢å‹˜éç¨‹ä¸­ç™¼å±•å‡ºä¾†çš„ã€‚

# æ­¥é©Ÿ 1ï¼šå•†æ¥­ç†è§£

# æ­¥é©Ÿ 2ï¼šè³‡æ–™ç†è§£(æ‘˜è¦,æ•˜è¿°æ€§çµ±è¨ˆåˆ†æ,è³‡æ–™è¦–è¦ºåŒ–,è³‡æ–™æ¸…é™¤,åˆä½µ,ç‰¹å¾µé¸æ“‡,è³‡æ–™è½‰æ›)
#   + è³‡æ–™è½‰æ›: æ­£è¦åŒ–(Normalization, L1, L2)
#       + L1: L1-norm is also known as least absolute deviations (LAD).
#       + L2: L2-norm is also known as least squares. 
#   + æ¨™æº–åŒ–(Standardization)
#       + (0,1)æ¨™æº–åŒ–.
#       + æœ€å°æœ€å¤§æ¨™æº–åŒ–(min-max normalization).
#       + Z-scoreæ¨™æº–åŒ–: å°‡ä»»æ„è³‡æ–™è½‰æ›ç‚ºè¶¨è¿‘å¹³å‡å€¼ç‚º0, æ¨™æº–å·®ç‚º1çš„åˆ†é….
#   + æ¨™ç±¤ç·¨ç¢¼ (Label encoding): YESè½‰æ›ç‚º1, NOè½‰æ›ç‚º0.
#   + æ•¸å€¼å‹è³‡æ–™è½‰æ›ç‚ºé¡åˆ¥å‹è³‡æ–™.
#   + ç¨ç†±ç·¨ç¢¼ (One-hot encoding): X1è½‰æ›ç‚º [1 0 0], X2è½‰æ›ç‚º [0 1 0], X3è½‰æ›ç‚º [0 0 1].

# æ­¥é©Ÿ 3ï¼šè³‡æ–™æº–å‚™ 
#   + å°‡è³‡æ–™éš¨æ©Ÿå€åˆ†ç‚ºäºŒå¤§é¡ï¼šè¨“ç·´é›†(train dataset), æ¸¬è©¦é›†(test dataset), æœ‰çš„æ¨¡å‹æœƒåŠ ä¸Šé©—è­‰é›†.
#   + è¨“ç·´é›†: è¨“ç·´æ¨¡å‹, ä½”æ•´é«”è³‡æ–™çš„70%
#   + é©—è­‰é›†: éƒ¨åˆ†æ–¹æ³•è€ƒæ…®å€åˆ†é©—è­‰é›† (validation dataset). ç”¨æ–¼è©•ä¼°æ¨¡å‹çš„åˆæ­¥åˆ¤æ–·èˆ‡è¶…åƒæ•¸èª¿æ•´.
#             å¦‚æœæ¨¡å‹ä¸å…·æœ‰è¶…åƒæ•¸, å‰‡ä¸ç”¨å€åˆ†æ­¤é©—è­‰é›†. è¶…åƒæ•¸: ä¾‹å¦‚é¡ç¥ç¶“ç¶²è·¯ä¸­éš±è—å–®å…ƒçš„æ•¸é‡.
#   + æ¸¬è©¦é›†: è©•ä¼°æ¨¡å‹, ä½”æ•´é«”è³‡æ–™çš„30%

# å€åˆ†æ¯”ä¾‹ç¯„ä¾‹1: è¨“ç·´é›† 70%, æ¸¬è©¦é›† 30%
# å€åˆ†æ¯”ä¾‹ç¯„ä¾‹2: è¨“ç·´é›† 80%, æ¸¬è©¦é›† 20%
# å€åˆ†æ¯”ä¾‹ç¯„ä¾‹3: è¨“ç·´é›† 80%, é©—è­‰é›† 10%, æ¸¬è©¦é›† 10%

# k æŠ˜äº¤å‰é©—è­‰ (k-fold cross validation)
# k-fold åŠƒåˆ†æ–¹æ³•å¯ä»¥é™ä½æ•¸æ“šåŠƒåˆ†å¸¶ä¾†çš„å½±éŸ¿.
# å°‡è³‡æ–™å€åˆ†æˆ k ä»½ï¼Œæ¯æ¬¡ä½¿ç”¨ (k-1) åšç‚ºè¨“ç·´é›†, å¦1ä»½åšç‚ºæ¸¬è©¦é›†ï¼Œè¨ˆç®— k æ¬¡çš„å¹³å‡äº¤å‰é©—è­‰æº–ç¢ºåº¦
# ä¸¦åšç‚ºæ¨¡å‹è©•ä¼°çš„æŒ‡æ¨™. ä¸€èˆ¬å¯ä»¥ä½¿ç”¨ k = 10.

# æ­¥é©Ÿ 4ï¼šæ¨¡å¼å»ºç«‹    (ä½¿ç”¨è¨“ç·´é›†)

# æ­¥é©Ÿ 5ï¼šè©•ä¼°èˆ‡æ¸¬è©¦  (ä½¿ç”¨æ¸¬è©¦é›†)

# æ­¥é©Ÿ 6ï¼šä½ˆç½²æ‡‰ç”¨

##############################
# 2.æ©Ÿå™¨å­¸ç¿’ç°¡ä»‹
##############################

# æ©Ÿå™¨å­¸ç¿’æ˜¯äººå·¥æ™ºæ…§çš„ä¸€å€‹åˆ†æ”¯.

# äººå·¥æ™ºæ…§çš„ç ”ç©¶æ­·å²: æ¨ç† --> çŸ¥è­˜ --> å­¸ç¿’.

# æ©Ÿå™¨å­¸ç¿’æ˜¯å¯¦ç¾äººå·¥æ™ºæ…§çš„ä¸€å€‹æ–¹æ³•,å³æ˜¯ä»¥æ©Ÿå™¨å­¸ç¿’ç‚ºæ‰‹æ®µè§£æ±ºäººå·¥æ™ºæ…§ä¸­çš„å•é¡Œ.

# æ©Ÿå™¨å­¸ç¿’åœ¨è¿‘30å¤šå¹´å·²ç™¼å±•ç‚ºä¸€é–€å¤šé ˜åŸŸäº¤å‰å­¸ç§‘,æ¶‰åŠæ©Ÿç‡è«–,çµ±è¨ˆå­¸,è¿‘ä¼¼ç†è«–,å‡¸åˆ†æ(æœ€ä½³åŒ–å•é¡Œ),
# è¨ˆç®—è¤‡é›œæ€§ç†è«–ç­‰å¤šé–€å­¸ç§‘.

# æ©Ÿå™¨å­¸ç¿’ç†è«–ä¸»è¦æ˜¯è¨­è¨ˆå’Œåˆ†æä¸€äº›è®“é›»è…¦å¯ä»¥è‡ªå‹•ã€Œå­¸ç¿’ã€çš„æ¼”ç®—æ³•.

# æ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•æ˜¯ä¸€é¡å¾è³‡æ–™ä¸­è‡ªå‹•åˆ†æç²å¾—è¦å¾‹,ä¸¦åˆ©ç”¨è¦å¾‹å°æœªçŸ¥è³‡æ–™é€²è¡Œé æ¸¬çš„æ¼”ç®—æ³•.

# å› ç‚ºå­¸ç¿’æ¼”ç®—æ³•ä¸­æ¶‰åŠäº†å¤§é‡çš„çµ±è¨ˆå­¸ç†è«–,æ©Ÿå™¨å­¸ç¿’èˆ‡æ¨è«–çµ±è¨ˆå­¸é—œä¿‚å¯†åˆ‡,ä¹Ÿè¢«ç¨±ç‚ºçµ±è¨ˆå­¸ç¿’ç†è«–.

# äººå·¥æ™ºæ…§ --> çµ±è¨ˆå­¸ç¿’ --> è³‡æ–™æ¢å‹˜ --> æ©Ÿå™¨å­¸ç¿’ (é æ¸¬+æ¼”ç®—æ³•) --> æ·±åº¦å­¸ç¿’ --> ç”Ÿæˆå¼AI

# å¸¸ç”¨æ©Ÿå™¨å­¸ç¿’åŒ…æ‹¬éç›£ç£å¼å­¸ç¿’èˆ‡ç›£ç£å¼å­¸ç¿’.

##############################
# 2.éç›£ç£å¼å­¸ç¿’èˆ‡ç›£ç£å¼å­¸ç¿’
##############################

# éç›£ç£å¼å­¸ç¿’ (unsupervised learning) åˆç¨±ç‚ºç„¡ç›£ç£å­¸ç¿’, æ˜¯æ©Ÿå™¨å­¸ç¿’çš„ä¸€ç¨®æ–¹æ³•,
# æ²’æœ‰çµ¦å®šäº‹å…ˆæ¨™è¨˜éçš„è¨“ç·´ç¯„ä¾‹(Y), è‡ªå‹•å°è¼¸å…¥çš„è³‡æ–™é€²è¡Œåˆ†ç¾¤.

# éç›£ç£å­¸ç¿’çš„ä¸»è¦é‹ç”¨åŒ…å«:
#   + é›†ç¾¤åˆ†æ (cluster analysis)
#   + é—œè¯è¦å‰‡ (association rule)
#   + ç¶­åº¦ç¸®æ¸› (dimensionality reduce). ä¾‹: ä¸»æˆåˆ†åˆ†æ (Principal Aomponents Analysis, PCA)

##############################
# 3.ç›£ç£å¼å­¸ç¿’
##############################

# ç›£ç£å¼å­¸ç¿’ (supervised learning): å·²çŸ¥ Y(äººç‚ºæ¨™è¨»çš„çµæœ, è¡¨ç¤ºæ¨™ç±¤), é€²è¡Œ X --> é æ¸¬ --> Y

# X: è‡ªè®Šæ•¸, ç¨ç«‹è®Šæ•¸ independent variable, é æ¸¬è®Šé‡ predictor variable, 
#    è§£é‡‹è®Šé‡ explanatory variable, å…±è®Šé‡ covariate.
# Y: åæ‡‰è®Šæ•¸ response variable, å› è®Šæ•¸, ä¾è®Šæ•¸, æ‡‰è®Šæ•¸, è¢«è§£é‡‹è®Šæ•¸ dependent variable, 
#    çµæœè®Šæ•¸ outcome variable.

# è¿´æ­¸åˆ†æ Regression analysis
# å»£ç¾©ç·šæ€§æ¨¡å‹ General linear model (GLM): é©ç”¨æ–¼Yç‚ºé¡åˆ¥å‹è®Šæ•¸æˆ–ç™¼ç”Ÿæ¬¡æ•¸è®Šæ•¸.
# æ±ºç­–æ¨¹ Decision tree
# å¤©çœŸè²æ°æ³• NaÃ¯ve-Bayes
# Kè¿‘é„°æ³• k-nearest neighbors (KNN)
# æ”¯æŒå‘é‡æ©Ÿ Support vector machine (SVM)
# éš¨æ©Ÿæ£®æ—æ³• Random forest
# é¡ç¥ç¶“ç¶²è·¯ Neural network (NN)

# é›†æˆå­¸ç¿’ (Ensemble learning): ä½¿ç”¨å¤šç¨®å­¸ç¿’ç®—æ¼”ç®—æ³•ä¾†ç²å¾—æ¯”ä½¿ç”¨å–®ä¸€æ¼”ç®—æ³•æ›´å¥½é æ¸¬çµæœ.
#   Bagging (Bootstrap AGGregatING) è£è¢‹æ³• (æ‹”é´é›†æˆæ³•)
#   Boosting æå‡æ³•
#   XGBoost (eXtreme Gradient Boosting) æ¥µç«¯æ¢¯åº¦æå‡æ³•

##############################
# 4.ç›£ç£å¼å­¸ç¿’çš„è©•ä¼°
##############################

# æ•¸å€¼æ¨¡å‹å¸¸ç”¨ç¸¾æ•ˆæŒ‡æ¨™:
# 1.å‡æ–¹èª¤å·® (Mean Squared Error, MSE) # https://en.wikipedia.org/wiki/Mean_squared_error
# 2.å‡æ–¹æ ¹èª¤å·® (Root Mean Squared Error, RMSE)
# 3.å¹³å‡çµ•å°èª¤å·® (Mean Absolute Error, MAE)

# é¡åˆ¥æ¨¡å‹ç¸¾æ•ˆæŒ‡æ¨™
# http://rwepa.blogspot.com/2013/01/rocr-roc-curve.html
# ç†è§£æ·†çŸ©é™£ (Confusion matrix)

##############################
# 5.scikit-learnd æ¨¡çµ„ç°¡ä»‹
##############################

# Scikit-learn æ˜¯ä½¿ç”¨ Python ç¨‹å¼èªè¨€æ’°å¯«çš„è‡ªç”±ä¸¦é–‹æºçš„æ©Ÿå™¨å­¸ç¿’æ¡†æ¶ä¹‹ä¸€
# æä¾›è±å¯Œçš„æ©Ÿå™¨å­¸ç¿’æ¼”ç®—æ³•å’Œå·¥å…·ï¼Œè®“ä½¿ç”¨è€…å¯ä»¥æ–¹ä¾¿åœ°é€²è¡Œæ•¸æ“šåˆ†æå’Œå»ºç«‹æ¨¡å‹ã€‚
# åœ¨æ©Ÿå™¨å­¸ç¿’æµç¨‹ä¸­ï¼Œç¶“å¸¸éœ€è¦é€²è¡Œè³‡æ–™å‰è™•ç†ã€ç‰¹å¾µé¸æ“‡å’Œæ¨¡å‹é¸æ“‡ç­‰ä¸€ç³»åˆ—æ­¥é©Ÿçš†åŒ…æ‹¬åœ¨ Scikit-learnã€‚
# Scikit-learn èˆ‡å…¶ä»– Python æ¨¡çµ„å¯ä»¥é€²è¡Œæ–¹ä¾¿ä½¿ç”¨ï¼Œä¾‹: 
#   + ç¹ªåœ–      matplotlib, plotly
#   + é™£åˆ—è¨ˆç®—  numpy
#   + è³‡æ–™æ¡†    pandas
#   + ç§‘å­¸è¨ˆç®—  scipy 

# scikit-learn å®˜ç¶²
# https://scikit-learn.org/stable/

# å…­å¤§åŠŸèƒ½
# 1.Classification           åˆ†é¡
# 2.Regression               è¿´æ­¸
# 3.Clustering               é›†ç¾¤
# 4.Dimensionality reduction ç¶­åº¦ç¸®æ¸›
# 5.Model selection          æ¨¡å‹é¸å–
# 6.Preprocessing            è³‡æ–™é è™•ç†

# 1.10.1. Classification
# https://scikit-learn.org/stable/modules/tree.html#classification

# ç¯„ä¾‹: scikit-learn - iris æ±ºç­–æ¨¹

# è¼‰å…¥æ¨¡çµ„
from sklearn.model_selection import train_test_split # è¨“ç·´é›†,æ¸¬è©¦é›†åˆ†å‰²
from sklearn.datasets import load_iris # è¼‰å…¥ iris è³‡æ–™é›†
from sklearn.tree import DecisionTreeClassifier # è¼‰å…¥æ±ºç­–æ¨¹
from sklearn import tree # è¼‰æ¨¹
from matplotlib import pyplot as plt # è¼‰å…¥ç¹ªåœ–

# è¼‰å…¥è³‡æ–™
iris = load_iris()

# ç‰©ä»¶å‹æ…‹
type(iris) # sklearn.utils._bunch.Bunch

# è¨­å®šè‡ªè®Šæ•¸ X
X = iris.data

# è¨­å®šåæ‡‰è®Šæ•¸ y
y = iris.target

# è¨“ç·´é›†,æ¸¬è©¦é›†åˆ†å‰²
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# å»ºç«‹æ±ºç­–æ¨¹æ¡†æ¶
clf = DecisionTreeClassifier(max_leaf_nodes=5, random_state=0)

# å»ºç«‹æ±ºç­–æ¨¹æ¨¡å‹
clf.fit(X_train, y_train)

# è¨­å®šç¹ªåœ–åæ‡‰è®Šæ•¸çš„æ¨™è¨˜
class_names = iris["target_names"]

# è¨­å®šç¹ªåœ–å¤§å°-è‹±å‹
plt.figure(figsize=(12, 12))

# ç¹ªè£½æ±ºç­–æ¨¹è¦–è¦ºåŒ–
tree.plot_tree(clf, fontsize=6, class_names=class_names)

# å„²å­˜æ±ºç­–æ¨¹
plt.savefig("iris_decision_tree.png", dpi=300)

plt.savefig('iris_decision_tree.pdf', dpi=300)
# end
